---
title: "Exploration of Coursera-SwiftKey Text Data"
author: "Kirsten Meeker"
date: "Aug 5, 2018"
output: html_document
---

Tasks to accomplish

    1. Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
    
    2. Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.
    
Questions to consider

    1. Some words are more frequent than others - what are the distributions of word frequencies? 
    see histogram and word clouds created using word frequencies below
    
    2. What are the frequencies of 2-grams and 3-grams in the dataset? 
    see histogram and word clouds created using n-gram frequencies below
    
    3. How do you evaluate how many of the words come from foreign languages?
    I've converted to "latin1", "UTF-8" when creating the corpus. Some things to try are:
    comparing with a reference dictionary, Google's cld libraries, Twitter for twitter sample.
    
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("text_stats3.R")
```


## Create some samples from the larger files 
```{r sample_data}
sample_file(dir="data/en_US/",filename="en_US.twitter.txt",sample_size=10000)
sample_file(dir="data/en_US/",filename="en_US.blogs.txt",sample_size=10000)
sample_file(dir="data/en_US/",filename="en_US.news.txt",sample_size=10000)
```


## Explore twitter data
In order to compute probabilities for the Markov chain we need to know
the count of n-grams which start with an entered word.

If there is no n-gram starting with a word, a next word can be selected from the corpus or a language dictionary. In order to compute the probability of each word we'll need their frequencies and total number.


```{r explore_twitter_data, message=FALSE}

num_lines <- read.table(pipe("C:\\Rtools\\bin\\wc -l .\\data\\en_US\\en_US.twitter.txt"))[[1]]

print(paste("total number of lines: ",num_lines)) # documents are lines

if (file.exists("outputs/sample_en_US.twitter.RData")) {
    load("outputs/sample_en_US.twitter.RData")
} else {
    
    ## read and process data files a chunk at a time and return sparse DTM's
    lc <- large_corpus_tfs(dir="data/en_US/", filename="sample_en_US.twitter.txt", 
                            read_chunk=1000)
    save(lc, file = "outputs/sample_en_US.twitter.RData")
}

# bag of words
term_summary2(lc[["wordFreq"]])

# 2-gram
term_summary2(lc[["ng2Freq"]])
```

## Explore blogs data
```{r explore_blogs_data, message=FALSE}

num_lines <- read.table(pipe("C:\\Rtools\\bin\\wc -l .\\data\\en_US\\en_US.blogs.txt"))[[1]]

print(paste("total number of lines: ",num_lines)) # documents are lines

if (file.exists("outputs/sample_en_US.blogs.RData")) {
    load("outputs/sample_en_US.blogs.RData")
} else {
    
    ## read and process data files a chunk at a time and return sparse DTM's
    lc <- large_corpus_tfs(dir="data/en_US/", filename="sample_en_US.blogs.txt", 
                            read_chunk=1000)
    save(lc, file = "outputs/sample_en_US.blogs.RData")
}

# bag of words
term_summary2(lc[["wordFreq"]])

# 2-gram
term_summary2(lc[["ng2Freq"]])
```

## Explore news data
```{r explore_news_data, message=FALSE}

num_lines <- read.table(pipe("C:\\Rtools\\bin\\wc -l .\\data\\en_US\\en_US.news.txt"))[[1]]

print(paste("total number of lines: ",num_lines)) # documents are lines

if (file.exists("outputs/sample_en_US.news.RData")) {
    load("outputs/sample_en_US.news.RData")
} else {
    
    ## read and process data files a chunk at a time and return sparse DTM's
    lc <- large_corpus_tfs(dir="data/en_US/", filename="sample_en_US.news.txt", 
                            read_chunk=1000)
    save(lc, file = "outputs/sample_en_US.news.RData")
}

# bag of words
term_summary2(lc[["wordFreq"]])

# 2-gram
term_summary2(lc[["ng2Freq"]])
```
